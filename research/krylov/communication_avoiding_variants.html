<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conjugate Gradient</title>
<meta name="description" content="The Conjugate Conjugate algorithm is a widely used method for solving Ax=b when A is positive definite. Mathematically equivalent variants have been developed to reduce global communication.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../../tc.ico" rel="shortcut icon" >
<link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>

<div id="contentContainer" class="article" >
<h1> Communication Avoiding Conjugate Gradient Algorithms
</h1>
<p class="authors"> Tyler Chen
</p>
<p>One of the main drawbacks to Conjugate gradient in a high performance setting is …..</p>

<h2 id="communication-bottlenecks-in-cg">Communication bottlenecks in CG</h2>

<p>Recall the standard Hestenes and Stifel CG implementation. In the below description, every block of code after a “<strong>set</strong>” must wait for the output from the previous block. Much of the algorithm is scalar and vector updates which are relatively cheap (in terms of floating point operations and communication). The most expensive computations each iteration are the matrix vector product, and the two inner products.</p>

<p><strong>Algorithm.</strong> (Hestenes and Stiefel Conjugate Gradient) <span class="math display">\[\begin{align*}

&amp;\textbf{procedure}\text{ HSCG}( A,b,x_0 ) 

\\[-.4em]&amp;~~~~\textbf{set } r_0 = b-Ax_0, \nu_0 = \langle r_0,r_0 \rangle, p_0 = r_0, s_0 = Ar_0, 

\\[-.4em]&amp;~~~~\phantom{\textbf{set }}a_0 = \nu_0 / \langle p_0,s_0 \rangle

\\[-.4em]&amp;~~~~\textbf{for } k=1,2,\ldots \textbf{:} 

\\[-.4em]&amp;~~~~~~~~\textbf{set } x_k = x_{k-1} + a_{k-1} p_{k-1} 

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} r_k = r_{k-1} - a_{k-1} s_{k-1} 

\\[-.4em]&amp;~~~~~~~~\textbf{set } \nu_{k} = \langle r_k,r_k \rangle, \textbf{ and } b_k = \nu_k / \nu_{k-1}

\\[-.4em]&amp;~~~~~~~~\textbf{set }p_k = r_k + b_k p_{k-1}

\\[-.4em]&amp;~~~~~~~~\textbf{set }s_k = A p_k

\\[-.4em]&amp;~~~~~~~~\textbf{set }\mu_k = \langle p_k,s_k \rangle, \textbf{ and } a_k = \nu_k / \mu_k

\\[-.4em]&amp;~~~~~\textbf{end for}

\\[-.4em]&amp;\textbf{end procedure}

\end{align*}\]</span></p>

<p>A matrix vector product requires <span class="math inline">\(\mathcal{O}(\text{nnz})\)</span> (number of nonzero) floating point operations, while an inner product requires <span class="math inline">\(\mathcal{O}(n)\)</span> operations. For many applications of CG, the number of nonzero entries is something like <span class="math inline">\(kn\)</span>, where <span class="math inline">\(k\)</span> relatively small. In these cases, the cost of floating point arithmetic for a matrix vector product and an inner product is roughly the same. On the other hand, the communication costs for the inner products are much lower.</p>

<p>matvec is usually sparse</p>

<p>inner products require “all reduce” i.e. collect information back from all the different processors/nodes would like to be able to overlap these as much as possible</p>

<h2 id="overlapping-inner-products">Overlapping inner products</h2>

<p>We would like to be able to <em>overlap</em> as many of the heavy computations as possible. However, in the current form, we need to wait for each of the previous computations before we are able to do a matrix vector product or an inner product.</p>

<p>Using our recurrences we can write, <span class="math display">\[

s_k = Ap_k = A(r_k + b_k p_{k-1}) 

= Ar_k + b_k s_{k-1}

\]</span></p>

<p>If we define the axillary vector <span class="math inline">\(w_k = Ar_k\)</span>, in exact arithmetic using this formula for <span class="math inline">\(s_k\)</span> will be equivalent to the original formula for <span class="math inline">\(s_k\)</span>. However, we can now compute <span class="math inline">\(w_k\)</span> as soon as we have <span class="math inline">\(r_k\)</span>. Therefore, the computation of <span class="math inline">\(\nu_k = \langle r_k,r_k \rangle\)</span> can be overlapped with the computation of <span class="math inline">\(w_k = Ar_k\)</span>.</p>

<p>These coefficient formulas seem to work better that CGCG..</p>

<p><strong>Algorithm.</strong> (Chronopoulos and Gear Conjugate Gradient) <span class="math display">\[\begin{align*}

&amp;\textbf{procedure}\text{ CGCG}( A,b,x_0 ) 

\\[-.4em]&amp;~~~~\textbf{set } r_0 = b-Ax_0, \nu_0 = \langle r_0,r_0 \rangle, p_0 = r_0, s_0 = Ar_0, 

\\[-.4em]&amp;~~~~\phantom{\textbf{set }}a_0 = \nu_0 / \langle p_0,s_0 \rangle

\\[-.4em]&amp;~~~~\textbf{for } k=1,2,\ldots \textbf{:} 

\\[-.4em]&amp;~~~~~~~~\textbf{set } x_k = x_{k-1} + a_{k-1} p_{k-1} 

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} r_k = r_{k-1} - a_{k-1} s_{k-1} 

\\[-.4em]&amp;~~~~~~~~\textbf{set } w_k = Ar_k 

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} \nu_{k} = \langle r_k,r_k \rangle, \textbf{ and } b_k = \nu_k / \nu_{k-1}

\\[-.4em]&amp;~~~~~~~~\textbf{set }\eta_k = \langle r_k, w_k \rangle, \textbf{ and } a_k = \nu_k / (\eta_k - (b_k/a_{k-1})\nu_k)

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} p_k = r_k + b_k p_{k-1}

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} s_k = w_k + b_k s_{k-1}

\\[-.4em]&amp;~~~~~\textbf{end for}

\\[-.4em]&amp;\textbf{end procedure}

\end{align*}\]</span></p>

<p><strong>Algorithm.</strong> (Ghysels and Vanroose Conjugate Gradient) <span class="math display">\[\begin{align*}

&amp;\textbf{procedure}\text{ CGCG}( A,b,x_0 ) 

\\[-.4em]&amp;~~~~\textbf{set } r_0 = b-Ax_0, \nu_0 = \langle r_0,r_0 \rangle, p_0 = r_0, s_0 = Ar_0, 

\\[-.4em]&amp;~~~~\phantom{\textbf{set }}w_0 = s_0, u_0 = Aw_0, a_0 = \nu_0 / \langle p_0,s_0 \rangle

\\[-.4em]&amp;~~~~\textbf{for } k=1,2,\ldots \textbf{:} 

\\[-.4em]&amp;~~~~~~~~\textbf{set } x_k = x_{k-1} + a_{k-1} p_{k-1} 

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} r_k = r_{k-1} - a_{k-1} s_{k-1} 

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} w_k = w_{k-1} - a_{k-1} u_{k-1}

\\[-.4em]&amp;~~~~~~~~\textbf{set } \nu_k = \langle r_k,r_k\rangle, \textbf{ and } b_k = \nu_k/\nu_{k-1}

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} \eta_{k} = \langle r_k,w_k \rangle, \textbf{ and } a_k = \nu_k / (\eta_k - (b_k/a_{k-1})\nu_k)

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} t_k = Aw_k

\\[-.4em]&amp;~~~~~~~~\textbf{set } p_k = r_k + b_k p_{k-1}

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} s_k = w_k + b_k s_{k-1}

\\[-.4em]&amp;~~~~~~~~\phantom{\textbf{set }} u_k = b_k u_{k-1}

\\[-.4em]&amp;~~~~~\textbf{end for}

\\[-.4em]&amp;\textbf{end procedure}

\end{align*}\]</span></p>

<p>fda</p>

<h2 id="fdas">fdas</h2>

<p class="footer">More about Krylov methods can be found <a href="./">here</a>.</p>
</div>
</body>
</html>
