<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Tyler Chen</title>
<meta name="description" content="I'm Tyler Chen, applied math PhD student at the University of Washington. Find out more about my research, teaching, and educational beliefs, and then get in contact with me.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../../tc.ico" rel="shortcut icon" >
<link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>

<div id="contentContainer">
<h1> The Conjugate Gradient Algorithm in Finite Precision
</h1>
<p class="authors"> Tyler Chen
</p>
<p>This page is a work in progress.</p>

<p>A key component of our derivations of the <a href="./arnoldi_lanczos.html">Lanczos</a> and <a href="./cg_derivation.html">Conjugate Gradient</a> methods was the orthogonality of certain basis vectors. In finite precision, our induction based arguments no longer hold, and so it’s reasonable to expect the algorithms will fail. That said, since you’re reading about these methods, they must somehow still be usable in practice. This turns out to be the case, and both methods are widely used for eignevalue problems and solving linear systems.</p>

<p>The first major progress in the analysis of the Lanczos algorithm was done by Chris Paige, who characterized the behavior of the method in finite precision. A similarly important analysis of Conjugate Gradient was done by Anne Greenbaum in her 1989 paper, <a href="https://www.sciencedirect.com/science/article/pii/0024379589902851">“Behavior of slightly perturbed Lanczos and conjugate-gradient recurrences”</a>. A big takeaway from Greenbaum’s analysis is that the error bound from the Chevyshev polynomials still holds in finite precision (to a close approximation). My goal here is to present the highlights of that paper.</p>

<h2 id="the-results">The results</h2>

<p><a href="./remez.html">Remez Algorithm</a></p>

<h2 id="some-conditions-for-the-analysis">Some conditions for the analysis</h2>

<p>CG is doing the Lanczos algorithm in disguise. In particular, normalizing the residuals from CG gives the vectors <span class="math inline">\(q_j\)</span> produced by the Lanczos algorithm, and combing the CG constants in the right way gives the coefficients for the three term Lanczos recurrence.</p>

<p>The analysis by Greenbaum requires that the finite precision Conjugate Gradient algorithm (viewed as the Lanczos algorithm) satisfy a few properties. Namely,</p>

<ul>

<li>the three term Lanczos recurrence is well satisfied</li>

<li>the Lanczos vectors have norm close to one</li>

<li>successive Lanczos vectors are nearly orthogonal</li>

</ul>

<p>As it turns out, nobody has actually ever proved that any of the Conjugate Variant methods used in practice actually satisfy these conditions (although some do numerically satisfy the conditions).</p>

<p class="footer">More about Krylov methods can be found <a href="./">here</a>.</p>
</div>
</body>
</html>
