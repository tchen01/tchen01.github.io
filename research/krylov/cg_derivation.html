<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conjugate Gradient</title>
<meta name="description" content="The Conjugate Conjugate algorithm is a widely used method for solving Ax=b when A is positive definite. While it's simple to state the algorithm, understanding where it comes from is not always so clear.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../../tc.ico" rel="shortcut icon" >
<link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>

<div id="contentContainer">
<h1> A derivation of the Conjugate Gradient Algorithm
</h1>
<p class="authors"> Tyler Chen
</p>
<p>There are many ways to view and derive the Conjugate Gradient algorithm. To me, this is of those topics where you have to go through the explanations a few times before you start to really understanding what is going on. I’ll derive the algorithm by directly minimizing by minimizing the <span class="math inline">\(A\)</span>-norm of the error over successive Krylov subspaces, <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>, which to me is the most natural way to think about the algorithm. My hope is that the derivation here provides an intuitive introduction to CG. Of course, what I think is a good way to present the topic won’t match up with ever reader’s own preference, so I highly recommend reading through some other resources as well.</p>

<h2 id="linear-algebra-review">Linear algebra review</h2>

<p>Before we get into the details, let’s define some notation and review a few key concepts from linear algebra which we will rely on when deriving the CG algorithm.</p>

<ul>

<li>Any inner product <span class="math inline">\(\langle \cdot,\cdot \rangle\)</span> induces a norm <span class="math inline">\(\|\cdot\|\)</span> defined by <span class="math inline">\(\|x\|^2 = \langle x,x\rangle\)</span>.</li>

<li>For the rest of this piece we will denote the standard (Euclidian) inner product by <span class="math inline">\(\langle \cdot,\cdot\rangle\)</span> and the (Euclidian) norm by <span class="math inline">\(\|\cdot\|\)</span> or <span class="math inline">\(\|\cdot\|_2\)</span>.</li>

<li>A martix <span class="math inline">\(A\)</span> is positive definite if <span class="math inline">\(\langle x, Ax\rangle &gt; 0\)</span> for all <span class="math inline">\(x\)</span>.</li>

<li>A symmetric positive definite matrix <span class="math inline">\(A\)</span> naturally induces the inner product <span class="math inline">\(\langle \cdot,\cdot \rangle_A\)</span> defined by <span class="math inline">\(\langle x,y\rangle_A = \langle x,Ay\rangle = \langle Ax,y \rangle\)</span>. The associated norm, called the <span class="math inline">\(A\)</span>-norm will be denoted by <span class="math inline">\(\langle \cdot,\cdot\rangle_A\)</span> and is defined by, <span class="math display">\[

\|x\|_A^2 = \langle x,x \rangle_A = \langle x,Ax \rangle = \| A^{1/2}x \|

\]</span></li>

<li>The point in a subspace <span class="math inline">\(V\)</span> nearest to a point <span class="math inline">\(x\)</span> is the projection of <span class="math inline">\(x\)</span> onto <span class="math inline">\(V\)</span> (where projection is done with the inner product and distance is measured with the induced norm). Given an orthonormal basis for <span class="math inline">\(V\)</span>, this amounts to summing the projection of <span class="math inline">\(x\)</span> onto each of the basis vectors.</li>

<li>The <span class="math inline">\(k\)</span>-th Krylov subspace generated by <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span> is, <span class="math display">\[

\mathcal{K}_k(A,b) = \operatorname{span}\{b,Ab,\ldots,A^{k-1}b\}

\]</span></li>

</ul>

<h2 id="minimizing-the-error">Minimizing the error</h2>

<p>Now that we have that out of the way, let’s begin our derivation. As stated above, we will minimize the <span class="math inline">\(A\)</span>-norm of the error over successive Krylov subspaces generated by <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span>. That is to say <span class="math inline">\(x_k\)</span> will be the point so that, <span class="math display">\[

\|e_k\|_A

:=\| x_k - x^* \|_A 

= \min_{x\in\mathcal{K}_k(A,b)} \| x - x^* \|_A, ~~~~ x^* = A^{-1}b

\]</span></p>

<p>Since we are minimizing with respect to the <span class="math inline">\(A\)</span>-norm, it will be useful to have an <span class="math inline">\(A\)</span>-orthonormal basis for <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>. That is, a basis which is orthonormal in the <span class="math inline">\(A\)</span>-inner product. For now, let’s just say we have such a basis, <span class="math inline">\(\{p_0,p_1,\ldots,p_{k-1}\}\)</span>, ahead of time. Since <span class="math inline">\(x_k\in\mathcal{K}_k(A,b)\)</span> we can write <span class="math inline">\(x_k\)</span> in terms of this basis, <span class="math display">\[

x_k = a_0 p_0 + a_1 p_1 + \cdots + a_{k-1} p_{k-1}

\]</span></p>

<p>Note that we have <span class="math inline">\(x_0 = 0\)</span> and <span class="math inline">\(e_k = x^* - x_k\)</span>. Then, <span class="math display">\[

e_k = e_0 - a_0p_0 - a_1 p_1 - \cdots - a_{k-1} p_{k-1}

\]</span></p>

<p>By definition, the coefficients for <span class="math inline">\(x_k\)</span> were chosen to minimize the <span class="math inline">\(A\)</span>-norm of the error, <span class="math inline">\(\|e_k\|_A\)</span>, over <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>. Therefore, <span class="math inline">\(e_k\)</span> has zero component in each of the directions <span class="math inline">\(\{ p_0,p_1,\ldots,p_{k-1} \}\)</span>. In particular, that means that <span class="math inline">\(a_jp_j\)</span> cancels exactly with <span class="math inline">\(e_0\)</span> in the direction of <span class="math inline">\(p_j\)</span>.</p>

<!-- is this part clear?? maybe need to explain more..

-->

<p>We now observe that since the coefficients <span class="math inline">\(a_0&#39;,a_1&#39;,\ldots,a_{k-2}&#39;\)</span> of <span class="math inline">\(x_{k-1}\)</span> were chosen in exactly the same way, so that <span class="math inline">\(a_0=a_0&#39;, a_1=a_1&#39;, \ldots, a_{k-2}=a_{k-2}&#39;\)</span>. Therefore, <span class="math display">\[

x_k = x_{k-1} + a_{k-1} p_{k-1}

\]</span> and <span class="math display">\[

e_k = e_{k-1} - a_{k-1} p_{k-1}

\]</span></p>

<p>Now that we have explicitly written <span class="math inline">\(x_k\)</span> in terms of an update to <span class="math inline">\(x_{k-1}\)</span> this is starting to look like an iterative method!</p>

<p>Let’s compute an explicit representation of the coefficient <span class="math inline">\(a_{k-1}\)</span>. As previously noted, we have chosen <span class="math inline">\(x_k\)</span> to minimize <span class="math inline">\(\|e_k\|_A\)</span> over <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>. Therefore, the component of <span class="math inline">\(e_k\)</span> in each of the directions <span class="math inline">\(p_0,p_1,\ldots,p_{k-1}\)</span> must be zero. That is, <span class="math inline">\(\langle e_k , p_j \rangle = 0\)</span> for all <span class="math inline">\(i=0,1,\ldots, k-1\)</span>. <span class="math display">\[

0 = \langle e_k , p_{k-1} \rangle_A

= \langle e_{k-1}, p_{k-1} \rangle - a_{k-1} \langle p_{k-1} , p_{k-1} \rangle_A

\]</span></p>

<p>Thus <span class="math display">\[

a_{k-1} 

= \frac{\langle e_{k-1}, p_{k-1} \rangle_A}{\langle p_{k-1},p_{k-1} \rangle_A} 

\]</span></p>

<p>This expression might look like a bit of a roadbock, since if we knew the initial error <span class="math inline">\(e_0 = x^* - 0\)</span> then we would know the solution to the original system! However, we have been working with the <span class="math inline">\(A\)</span>-inner product so we can write, <span class="math display">\[

Ae_{k-1} = A(x^* - x_{k-1}) = b - Ax_{k-1} = r_{k-1}

\]</span> Therefore, we can compute <span class="math inline">\(a_{k-1}\)</span> as, <span class="math display">\[

a_{k-1}

= \frac{\langle r_{k-1}, p_{k-1} \rangle}{\langle p_{k-1},A p_{k-1} \rangle} 

\]</span></p>

<!--

Thus, minimizing the $A$-norm of the error, $\|e_k\|_A$ over $\mathcal{K}_k(A,b)$ amounts to projecting $e_k$ into the $A$-orthonormal basis $\{p_0, p_1, \ldots, p_{k-1} \}$.

This means that $e_k$ has zero component in each of the directions $\{p_0,p_1,\ldots,p_{k-1}\}$.

Since $\langle p_i,p_j\rangle_A = 0$ if $i\neq j$ we have,

$$

0 = \langle e_k,p_j \rangle_A 

= \langle e_0, p_j \rangle_A - a_j \langle p_j ,p_j \rangle_A

$$



Therefore, $a_{j} = \langle e_0,p_j \rangle_A / \langle p_j,p_j \rangle_A$. 

This might look like a bit of a roadbock because if we knew the initial error $e_0 = x^* - 0$ then we would have known the solution to the original system! This is where the fact that we have been working with the $A$-inner product comes in handy.

Notice that,

$$

Ae_k = A(x^* - x_k) = b - Ax_k = r_k

$$



Thus, since $r_0 = b$, we can write $a_j = \langle b,p_j \rangle/\langle p_j,Ap_j\rangle$.



This looks much more promising.

Not only does it give us an easy way to compute $a_k$ at each step, but we can reuse all the coefficients we have computed at previous steps.

In particular,

$$

x_k = x_{k-1} + a_{k-1}p_{k-1}

$$

and redoing our error computation,

$$

a_{k-1} = \frac{\langle r_{k-1}, p_{k-1} \rangle}{\langle p_{k-1}, Ap_{k-1} \rangle}

$$

-->

<h2 id="finding-the-search-directions">Finding the Search Directions</h2>

<p>At this point we are almost done. The last thing to do is understand how to update <span class="math inline">\(p_k\)</span>. The first thing we might try would be to do something like Gram-Schmidt on <span class="math inline">\(\{b,Ab,A^2b,\ldots \}\)</span> to get the <span class="math inline">\(p_k\)</span>, i.e. Arnoldi iteration in the inner product induced by <span class="math inline">\(A\)</span>. This will work fine if you take some care with the exact implementation. However, since <span class="math inline">\(A\)</span> is symmetric we might hope to be able to use some short recurrence, which turns out to be the case.</p>

<p>Since <span class="math inline">\(r_k = b-Ax_k\)</span> and <span class="math inline">\(x_k\in\mathcal{K}_k(A,b)\)</span>, then <span class="math inline">\(r_k \in \mathcal{K}_{k+1}(A,b)\)</span>. Thus, we can obtain <span class="math inline">\(p_k\)</span> by <span class="math inline">\(A\)</span>-orthogonalizing <span class="math inline">\(r_k\)</span> against <span class="math inline">\(\{p_0,p_1,\ldots,p_{k-1}\}\)</span>.</p>

<p>Recall that <span class="math inline">\(e_k\)</span> is <span class="math inline">\(A\)</span>-orthogonal to <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>. That is, for <span class="math inline">\(j\leq k-1\)</span>, <span class="math display">\[

\langle e_k, A^j b \rangle_A = 0

\]</span></p>

<p>Therefore, noting that <span class="math inline">\(Ae_k = r_k\)</span>, for <span class="math inline">\(j\leq k-2\)</span>, <span class="math display">\[

\langle r_k, A^j b \rangle_A = 0

\]</span></p>

<p>That is, <span class="math inline">\(r_k\)</span> is <span class="math inline">\(A\)</span>-orthogonal to <span class="math inline">\(\mathcal{K}_{k-1}(A,b)\)</span>. In particular, this means that, for <span class="math inline">\(j\leq k-2\)</span>, <span class="math display">\[

\langle r_k, p_j \rangle_A = 0

\]</span></p>

<p>That means that to obtain <span class="math inline">\(p_k\)</span> we really only need to <span class="math inline">\(A\)</span>-orthogonalize <span class="math inline">\(r_k\)</span> against <span class="math inline">\(p_{k-1}\)</span>! That is, <span class="math display">\[

p_k = r_k + b_k p_{k-1}, ~~~~ b_k = - \frac{\langle r_k, p_{k-1} \rangle_A}{\langle p_{k-1}, p_{k-1} \rangle_A}

\]</span></p>

<p>The immediate consequence is that we do not need to save the entire basis <span class="math inline">\(\{p_0,p_1,\ldots,p_{k-1}\}\)</span>, but instead can just keep <span class="math inline">\(x_k\)</span>,<span class="math inline">\(r_k\)</span>, and <span class="math inline">\(p_{k-1}\)</span>. <strong>expand on this</strong>!!</p>

<h2 id="putting-it-all-together">Putting it all together</h2>

<p>We are now essentially done! In practice, people generally use the following equivalent formulas for <span class="math inline">\(a_{k-1}\)</span> and <span class="math inline">\(b_k\)</span>, <span class="math display">\[

a_{k-1} = \frac{\langle r_{k-1},r_{k-1}\rangle}{\langle p_{k-1},Ap_{k-1}\rangle}, ~~~~ b_k = \frac{\langle r_k,r_k\rangle}{\langle r_{k-1},r_{k-1}\rangle}

\]</span></p>

<p>We can now put everything together and <a href="./cg.py">implement</a> it in numpy. Note that we use <span class="math inline">\(f\)</span> for the right hand side vector to avoid conflict with the coefficient <span class="math inline">\(b\)</span>.</p>

<pre><code>def cg(A,f,max_iter):

    x = np.zeros(len(f)); r = np.copy(f); p = np.copy(r); s=A@p

    nu = r @ r; a = nu/(p@s); b = 0

    for k in range(1,max_iter):

        x += a*p

        r -= a*s



        nu_ = nu

        nu = r@r

        b = nu/nu_



        p = r + b*p

        s = A@p



        a = nu/(p@s)



    return x</code></pre>

<p class="footer">More about Krylov methods can be found <a href="./">here</a>.</p>
</div>
</body>
</html>
