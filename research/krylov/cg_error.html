<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Conjugate Gradient</title>
<meta name="description" content="The Conjugate Conjugate algorithm is a widely used method for solving Ax=b when A is positive definite. Characterizing the convergence of CG is important for understanding the rescources the algorithm will require.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../../tc.ico" rel="shortcut icon" >
<link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>

<div id="contentContainer">
<h1> Error Bounds for the Conjugate Gradient Algorithm
</h1>
<p class="authors"> Tyler Chen
</p>
<p>This page is a work in progress.</p>

<p>In our <a href="./cg_derivation.html">derivation</a> of the Conjugate Gradient method, we minimized the <span class="math inline">\(A\)</span>-norm of the error over sucessive Krylov subspaces. Ideally we would like to know how quickly this method converge. That is, how many iterations are needed to reach a specified level of accuracy.</p>

<h2 id="linear-algebra-review">Linear algebra review</h2>

<ul>

<li>The 2-norm of a symmetric positive definite matrix is the largest eigenvalue of the matrix</li>

<li>The 2-norm is submultiplicative. That is, <span class="math inline">\(\|A\|\|B\|\leq \|AB\|\)</span></li>

<li>A matrix <span class="math inline">\(U\)</span> is called unitary if <span class="math inline">\(U^*U = UU^* = I\)</span>.</li>

</ul>

<h2 id="polynomial-error-bounds">Polynomial error bounds</h2>

<p>Previously we have show that, <span class="math display">\[

e_k \in e_0 +  \operatorname{span}\{p_0,p_1,\ldots,p_{k-1}\} = e_0 + \mathcal{K}_k(A,b)

\]</span></p>

<p>Observing that <span class="math inline">\(r_0 = Ae_0\)</span> we find that, <span class="math display">\[

e_k \in e_0 +  \operatorname{span}\{Ae_0,A^2e_0,\ldots,A^{k}e_0\}

\]</span></p>

<p>Thus, we can write, <span class="math display">\[

\| e_k \|_A =  \min_{p\in\mathcal{P}_k}\|p(A)e_0\|_A , ~~~~ \mathcal{P}_k = \{p : p(0) = 1, \operatorname{deg} p \leq k\}

\]</span></p>

<p>Since <span class="math inline">\(A^{1/2} p(A) = p(A)A^{1/2}\)</span> we can write, <span class="math display">\[

\| p(A)e_0 \|_A

= \|A^{1/2} p(A)e_0 \|

= \|p(A) A^{1/2}e_0 \|

\]</span></p>

<p>Now, using the submultiplicative property of the 2-norm, <span class="math display">\[

\|p(A) A^{1/2}e_0 \|

\leq \|p(A)\| \|A^{1/2} e_0 \|

= \|p(A)\| \|e_0\|_A

\]</span></p>

<p>Since <span class="math inline">\(A\)</span> is positive definite, it is diagonalizable as <span class="math inline">\(U\Lambda U^*\)</span> where <span class="math inline">\(U\)</span> is unitary and <span class="math inline">\(\Lambda\)</span> is the diagonal matrix of eigenvalues of <span class="math inline">\(A\)</span>. Thus, <span class="math display">\[

A^k = (U\Lambda U^*)^k = U\Lambda^kU^*

\]</span></p>

<p>We can then write <span class="math inline">\(p(A) = Up(\Lambda)U^*\)</span> where <span class="math inline">\(p(\Lambda)\)</span> has diagonal entries <span class="math inline">\(p(\lambda_i)\)</span>. Therefore, using the <em>unitary invariance</em> property of the 2-norm, <span class="math display">\[

\|p(A)\| = \|Up(\Lambda)U^*\| = \|p(\Lambda)\|

\]</span></p>

<p>Now, since the 2-norm of a symmetric matrix is the magnitude of the largest eigenvalue, <span class="math display">\[

\| p(\Lambda) \| = \max_i |p(\lambda_i)|

\]</span></p>

<p>Finally, putting everything together we have, <span class="math display">\[

\frac{\|e_k\|_A}{\|e_0\|_A} \leq \min_{p\in\mathcal{P}_k} \max_i |p(\lambda_i)|

\]</span></p>

<p>Since the inequality we obtained from the submultiplicativity of the 2-norm is tight, this bound is also tight in the sense that for a fixed <span class="math inline">\(k\)</span> there exists an initial error <span class="math inline">\(e_0\)</span> so that equality holds.</p>

<p>Computing the optimal <span class="math inline">\(p\)</span> is not trivial, but an algorithm called the <a href="./remez.html">Remez algorithm</a> can be used to compute it.</p>

<p>Let <span class="math inline">\(L\subset \mathbb{R}\)</span> be some closed set. The <em>minimax polynomial of degree <span class="math inline">\(k\)</span></em> on <span class="math inline">\(L\)</span> is the polynomial satisfying, <span class="math display">\[

\min_{p\in\mathcal{P}_k} \max_{x\in L} | p(x) |, ~~~~ \mathcal{P}_k = \{p : p(0)=1, \deg p \leq k\}

\]</span></p>

<h3 id="chebyshev-bounds">Chebyshev bounds</h3>

<p>The minimax polynomial on the eigenvalues of <span class="math inline">\(A\)</span> is a bit tricky to work with. Although we can find it using the Remez algorithm, this is somewhat tedious, and requires knowledge of the whole spectrum of <span class="math inline">\(A\)</span>. We would like to come up with a bound which depends on less informationabout <span class="math inline">\(A\)</span>. One way to obtain such a bound is to expand the set on which we are looking for the minimax polynomial.</p>

<p>To this end, let <span class="math inline">\(\mathcal{I} = [\lambda_{\text{min}},\lambda_{\text{max}}]\)</span>. Then, since <span class="math inline">\(\lambda_i\in\mathcal{I}\)</span>, <span class="math display">\[

\min_{p\in\mathcal{P}_k} \max_i |p(\lambda_i)| 

\leq \min_{p\in\mathcal{P}_k} \max_{x \in \mathcal{I}} |p(x)| 

\]</span></p>

<p>The right hand side requires that we know the largest and smallest eigenvalues of <span class="math inline">\(A\)</span>, but doesnâ€™t require any of the ones between. This means it can be useful in practice, since we can easily compute the top and bottom eignevalues with the power method.</p>

<p>The polynomials satisfying the right hand side are called the <em>Chebyshev Polynomials</em> and can be easily written down with a simple recurrence relation. If <span class="math inline">\(\mathcal{I} = [-1,1]\)</span> then the relation is, <span class="math display">\[

T_{k+1}(x) = 2xT_k(x) - T_{k-1}(x), ~~~~ T_0=1,~~~~ T_1=x

\]</span></p>

<p>For <span class="math inline">\(\mathcal{I} \neq [-1,1]\)</span>, the above polynomials are simply stretched and shifted to the interval in question.</p>

<p>Let <span class="math inline">\(\kappa = \lambda_{\text{max}} / \lambda_{\text{min}}\)</span> (this is called the condition number). Then, from properties of these polynomials, <span class="math display">\[

\frac{\|e_k\|_A}{\|e_0\|_A} \leq 2 \left( \frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1} \right)^k

\]</span></p>

<p class="footer">More about Krylov methods can be found <a href="./">here</a>.</p>
</div>
</body>
</html>
