<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <title>Conjugate Gradient is Lanczos in Disguise</title>
  <meta name="description" content="The Conjugate Gradient and Lanczos algorithms are both widely used Krylov subspace methods for positive definite matrices. In fact, the CG algorithm generates a three term Lanczos recurrence.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
  <meta name="author" content="Tyler Chen" />
  <meta name="keywords" content="applied, math" />

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link href="../../tc.ico" rel="shortcut icon" >
  <link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
  <link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
  <link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />
  <link rel="stylesheet" href="../../font/vollkorn/stylesheet.css" type="text/css" charset="utf-8" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>

</head>
<body>
<div id="contentContainer">
<h1>Conjugate Gradient is Lanczos in Disguise</h1>
<p class="author"><a href="https://chen.pw">Tyler Chen</a></p>
<p>It’s perhaps not so surprising that the conjugate gradient and Lanczos algorithms are closely related. After all, they are both Krylov subspace methods for symmetric matrices.</p>
<p>More precisely, the Lanczos algorithm will produce an orthonormal basis for <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>, <span class="math inline">\(k=0,1,\ldots\)</span> if we initialize with initial vector <span class="math inline">\(r_0 = b\)</span>.
In our <a href="./cg_derivation.html">derivation</a> of the conjugate gradient algorithm, we saw that the updated residuals form an orthogonal basis for for these spaces.
This means the Lanczos vectors must be scaled versions of the conjugate gradient residuals.</p>
<p>The relationship between the conjugate gradient and Lanczos algorithms provides a way of transferring results about one algorithm to the other.
In fact, the analysis of <a href="./finite_precision_cg.html">finite precision CG</a> done by Greenbaum requires viewing CG in terms of the Lanczos recurrence.</p>
<p>In case you’re just looking for the punchline, the Lanczos vectors and coefficients can be obtained from the conjugate gradient algorithm by the following relationship,
<span class="math display">\[\begin{align*}
q_{j+1} \equiv (-1)^j\dfrac{r_j}{\|r_j\|}
,&amp;&amp;
\beta_j \equiv \frac{\|r_j\|}{\|r_{j-1}\|}\frac{1}{a_{j-1}}
,&amp;&amp;
\alpha_j \equiv \left(\frac{1}{a_{j-1}} + \frac{b_{j}}{a_{j-2}}\right)
\end{align*}\]</span></p>
<p>Note that the indices are offset, because the Lanczos algorithm is started with initial vector <span class="math inline">\(q_1\)</span>.</p>
<h2>Derivation</h2>
<p>The derivation of the above result is not particularly difficult although it is somewhat tedious.
Before you read my derivation, I would highly recommend trying to derive it on your own, since it’s a good chance to improve your familiarity with both algorithms.
While I hope that my derivation is not too hard to follow, there are definitely other ways to arrive at the same result, and often to really start to understand something you have to work it out on your own.</p>
<p>Recall that the Lanczos algorithm gives the three term recurrence,
<span class="math display">\[\begin{align*}
Aq_j = \alpha_j q_j + \beta_{j-1}q_{j-1} + \beta_j q_{j+1}
\end{align*}\]</span></p>
<p>In each iteration of CG we update,
<span class="math display">\[\begin{align*}
r_j = r_{j-1} - a_{j-1} Ap_{j-1}
,&amp;&amp;
p_j = r_j + b_j p_{j-1}
\end{align*}\]</span></p>
<p>Thus, substituting the expression for <span class="math inline">\(p_{j-1}\)</span> we find,
<span class="math display">\[\begin{align*}
r_j &amp;= r_{j-1} - a_{j-1} A(r_{j-1} + b_{j-1} p_{j-2})
\\&amp;= r_{j-1} - a_{j-1} Ar_{j-1} - a_{j-1}b_{j-1} A p_{j-2}
\end{align*}\]</span></p>
<p>Tearranging our equation for <span class="math inline">\(r_{j-1}\)</span> we have that <span class="math inline">\(Ap_{j-2} = (r_{j-2} - r_{j-1}) / a_{j-2}\)</span> so that,
<span class="math display">\[\begin{align*}
    r_j &amp;= r_{j-1} - a_{j-1} Ar_{j-1} - \frac{a_{j-1}b_{j-1}}{a_{j-2}}(r_{j-2} - r_{j-1})
\end{align*}\]</span></p>
<p>At this point we’ve found a three term recurrence for <span class="math inline">\(r_j\)</span>, which is a hopeful sign that we are on the right track.
We know that the Lanczos vectors are orthonormal and that the recurrence is symmetric, so we can keep massaging our CG relation to try to get it into that form.</p>
<p>First, let’s rearrange terms and regroup them so that the indices and matrix multiply match up with the Lanczos recurrence. This gives,
<span class="math display">\[\begin{align*}
    Ar_{j-1} = \left(\frac{1}{a_{j-1}}+\frac{b_{j-1}}{a_{j-2}}\right) r_{j-1} - \frac{b_{j-1}}{a_{j-2}} r_{j-2} - \frac{1}{a_{j-1}} r_{j}
\end{align*}\]</span></p>
<p>Now, we normalize our residuals as <span class="math inline">\(z_j = r_{j-1}/\|r_{j-1}\|\)</span> so that <span class="math inline">\(r_{j-1} = \|r_{j-1}\| z_j\)</span>. Plugging these in gives,
<span class="math display">\[\begin{align*}
    \|r_{j-1}\|Az_{j} &amp;= \|r_{j-1}\|\left(\frac{1}{a_{j-1}}+\frac{b_{j-1}}{a_{j-2}}\right) z_{j} 
    -\|r_{j-2}\|\frac{b_{j-1}}{a_{j-2}} z_{j} - \|r_j\| \frac{1}{a_{j-1}} z_{j+1}
\end{align*}\]</span></p>
<p>Dividing through by <span class="math inline">\(\|r_{j-1}\|\)</span> we have,
<span class="math display">\[\begin{align*}
    Az_{j} &amp;= \left(\frac{1}{a_{j-1}}+\frac{b_{j-1}}{a_{j-2}}\right) z_{j} 
    - \frac{\|r_{j-2}\|}{\|r_{j-1}\|}\frac{b_{j-1}}{a_{j-2}} z_{j-1} - \frac{\|r_j\|}{\|r_{j-1}\|} \frac{1}{a_{j-1}} z_{j+1}
\end{align*}\]</span></p>
<p>This looks close, but the coefficients for the last two terms should have the same formula. However, recall that <span class="math inline">\(b_{j} = \langle r_j,r_j \rangle / \langle r_{j-1},r_{j-1} \rangle = \|r_j\|^2 / \|r_{j-1}\|^2\)</span>. Thus,
<span class="math display">\[\begin{align*}
    Az_{j} &amp;= \left(\frac{1}{a_{j-1}}-\frac{b_{j-1}}{a_{j-2}}\right) z_{j} 
    - \frac{\|r_{j-1}\|}{\|r_{j-2}\|}\frac{1}{a_{j-2}} z_{j-1} - \frac{\|r_j\|}{\|r_{j-1}\|} \frac{1}{a_{j-1}} z_{j+1}
\end{align*}\]</span></p>
<p>We’re almost there! While we have the correct for the recurrence, the coefficients from the Lanczos method are always positive. This means that our <span class="math inline">\(z_{j}\)</span> have the wrong signs. Fixing this gives the relationship,
<span class="math display">\[\begin{align*}
q_{j+1} \equiv (-1)^j\dfrac{r_j}{\|r_j\|}
,&amp;&amp;
\beta_j \equiv \frac{\|r_j\|}{\|r_{j-1}\|}\frac{1}{a_{j-1}}
,&amp;&amp;
\alpha_j \equiv \left(\frac{1}{a_{j-1}} + \frac{b_{j}}{a_{j-2}}\right)
\end{align*}\]</span></p>
<!--start_pdf_comment-->
<p>Next: <a href="./cg_error.html">Error Bounds for the conjugate gradient Algorithm</a>
<!--end_pdf_comment--></p>
<p class="footer">
More about the conjugate gradient method can be found <a href="./">here</a>.
</p>
</div>
</body>
</html>
