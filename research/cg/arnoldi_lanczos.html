<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Arnoldi/Lanczos Algorithms</title>
<meta name="description" content="The Arnoldi and Lanczos algorithms for computing an orthonormal basis for Krylov subspaces are at the core of most Krylov subspace methods.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../../tc.ico" rel="shortcut icon" >
<link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />
<link rel="stylesheet" href="../../font/vollkorn/stylesheet.css" type="text/css" charset="utf-8" />

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>

<div id="contentContainer"  >
<h1> The Arnoldi and Lanczos algorithms
</h1>
<p class="authors"> Tyler Chen
</p>
<p>The Arnoldi and Lanczos algorithms for computing an orthonormal basis for Krylov subspaces are, in one way or another, at the core of all Krylov subspace methods. Essentially, these algorithms are the Gram-Schmidt procedure applied to the vectors <span class="math inline">\(\{v,Av,A^2v,A^3v,\ldots\}\)</span> in clever ways. The Arnoldi algorithm works for any matrix, and the Lanczos algorithm works for Hermitian matrices.</p>
<h2 id="the-arnoldi-algorithm">The Arnoldi algorithm</h2>
<p>Recall that given a set of vectors <span class="math inline">\(\{v_1,v_2,\ldots, v_k\}\)</span> the Gram-Schmidt procedure computes an orthonormal basis <span class="math inline">\(\{q_1,q_2,\ldots,q_k\}\)</span> so that for all <span class="math inline">\(j\leq k\)</span>, <span class="math display">\[
\operatorname{span}\{v_1,\ldots,v_j\} = \operatorname{span}\{q_1,\ldots,q_j\}
\]</span></p>
<p>In short, at step <span class="math inline">\(j\)</span>, <span class="math inline">\(v_{j+1}\)</span> is orthogonalized against each of <span class="math inline">\(\{q_1,q_2,\ldots, q_j\}\)</span>.</p>
<p>If we tried to compute the set <span class="math inline">\(\{v,Av,A^2v,\ldots\}\)</span>, it would become very close to linearly dependent (and with rounding errors essentially numerically linearly dependent). This is because this basis is essentially the <a href="https://en.wikipedia.org/wiki/Power_iteration">power method</a>. The trick behind the Arnoldi algorithm is the fact that you do not need to construct the whole set <span class="math inline">\(\{v,Av,A^2v,\ldots\}\)</span> ahead of time. This allows us to come up with a basis for <span class="math inline">\(\{v,Av,A^2v,\ldots\}\)</span> in a more “stable” way.</p>
<p>Suppose at the beginning of step <span class="math inline">\(k\)</span> that we have already computed a basis <span class="math inline">\(\{q_1,q_2,\ldots,q_{k-1}\}\)</span> which has the same span as <span class="math inline">\(\{v,Av,\ldots, A^{k-2}v\}\)</span>. If we were doing Gram-Schmidt, then we would obtain <span class="math inline">\(q_k\)</span> by orthogonalizing <span class="math inline">\(A^{k-1}v\)</span> against each of the vectors in the basis <span class="math inline">\(\{q_1,q_2, \ldots, q_{k-1}\}\)</span>. In the Arnoldi algorithm we instead orthogonalize <span class="math inline">\(Aq_{k-1}\)</span> against <span class="math inline">\(\{q_1,q_2,\ldots, q_{k-1}\}\)</span>.</p>
<p>Let’s understand why these are the same. First, since the span of <span class="math inline">\(\{q_1,q_2,\ldots, q_{k-1}\}\)</span> is equal to the span of <span class="math inline">\(\{v,Av,\ldots, A^{k-2}v\}\)</span>, then <span class="math inline">\(q_{k-1}\)</span> can be written as a linear combination of <span class="math inline">\(\{v,Av,\ldots, A^{k-2}v\}\)</span>. That is, there exists coefficients <span class="math inline">\(c_i\)</span> such that, <span class="math display">\[
q_{k-1} = c_1v + c_2Av + \cdots + c_{k-1} A^{k-2}v
\]</span></p>
<p>Therefore, multiplying by <span class="math inline">\(A\)</span> we have, <span class="math display">\[
Aq_{k-1} = c_1Av + c_2A^2v + \cdots c_{k-1}A^{k-1}v
\]</span></p>
<p>Now, since each of <span class="math inline">\(\{Av,A^2v,\ldots, A^{k-2}v\}\)</span> are in the span of <span class="math inline">\(\{q_1,q_2,\ldots, q_{k-1}\}\)</span>, each of these components will disappear when we orthogonalize <span class="math inline">\(Aq_{k-1}\)</span> against <span class="math inline">\(\{q_1,q_2,\ldots,q_{k-1}\}\)</span>. This gives a vector in the same direction as the vector we get by orthogonalizing <span class="math inline">\(A^{k-1}v\)</span> against <span class="math inline">\(\{q_1,q_2,\ldots,q_{k-1}\}\)</span>. Since we get <span class="math inline">\(q_k\)</span> by normalizing the resulting vector, using <span class="math inline">\(Aq_{k-1}\)</span> will give us the same value for <span class="math inline">\(q_k\)</span> as using <span class="math inline">\(A^{k-1}v\)</span>.</p>
<p>The Arnoldi algorithm gives the relationship, <span class="math display">\[
AQ_k = Q_k H_k + h_{k+1,k} q_{k+1} \xi_k^{\mathsf{T}}
\]</span> where <span class="math inline">\(Q_k = [q_1,q_2,\ldots,q_k]\)</span> is the <span class="math inline">\(n\times k\)</span> matrix whose columns are <span class="math inline">\(\{q_1,q_2,\ldots,q_k\}\)</span>, <span class="math inline">\(H_k\)</span> is a <span class="math inline">\(k\times k\)</span> <a href="https://en.wikipedia.org/wiki/Hessenberg_matrix"><em>Upper Hessenburg</em></a> matrix, and <span class="math inline">\(\xi_k^{\mathsf{T}} = [0,\ldots,0,1]^{\mathsf{T}}\)</span> is the <span class="math inline">\(k\)</span>-th unit vector.</p>
<h3 id="ritz-vectors">Ritz vectors</h3>
<p>For instance, suppose that <span class="math inline">\(H_nv = \lambda v\)</span>. Then, <span class="math display">\[
A(Q_nv) = (Q_nH_nQ_n^{\mathsf{H}})(Q_nv) = Q_nH_nv = Q_n(\lambda v) = \lambda (Q_nv)
\]</span></p>
<p>This proves that if <span class="math inline">\(v\)</span> is an eigenvector of <span class="math inline">\(H_n\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(Q_nv\)</span> is an eigenvector of <span class="math inline">\(A\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>.</p>
<p>We have just seen that if <span class="math inline">\(Q_n\)</span> is unitary, then if <span class="math inline">\(v\)</span> is an eigenvector of <span class="math inline">\(H_n\)</span> then <span class="math inline">\(Q_nv\)</span> is an eigenvalue of <span class="math inline">\(A\)</span> when <span class="math inline">\(v\)</span>.</p>
<p>When <span class="math inline">\(k&lt;n\)</span> then <span class="math inline">\(Q_k\)</span> although <span class="math inline">\(Q_k\)</span> has orthonormal columns, it is not square. Even so, we can use <span class="math inline">\(Q_kv\)</span> as an “approximate” eigenvector of <span class="math inline">\(Q\)</span>.</p>
<p>More specifically, if <span class="math inline">\(v\)</span> is an eigenvector of <span class="math inline">\(H_k\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, then <span class="math inline">\(Q_kv\)</span> is called a <em>Ritz vector</em>, and <span class="math inline">\(\lambda\)</span> is called a <em>Ritz value</em>.</p>
<h2 id="the-lancozs-algorithm">The Lancozs algorithm</h2>
<p>When <span class="math inline">\(A\)</span> is Hermitian, then <span class="math inline">\(Q_k^{\mathsf{H}}AQ_k = H_k\)</span> is also Hermetian. This means that <span class="math inline">\(H_k\)</span> is upper Hessenburg and Hermitian, so it must be tridiagonal! Thus, the <span class="math inline">\(q_j\)</span> satisfy a three term recurrence, <span class="math display">\[
Aq_j = \beta_{j-1} q_{j-1} + \alpha_j q_j + \beta_j q_{j+1}
\]</span> which we can write in matrix form as, <span class="math display">\[
AQ_k = Q_k T_k + \beta_k q_{k+1} \xi_k^{\mathsf{T}}
\]</span></p>
<p>The Lanczos algorithm is an efficient way of computing this decomposition which takes advantage of the three term recurrence.</p>
<p>I will present a brief derivation for the method motivated by the three term recurrence above. Since we know that the <span class="math inline">\(q_j\)</span> satisfy the three term recurrence, we would like the method to store as few of the <span class="math inline">\(q_j\)</span> as possible (i.e. take advantage of the three term recurrence as opposed to the Arnoldi algorithm).</p>
<p>Suppose that we have <span class="math inline">\(q_j\)</span>, <span class="math inline">\(q_{j-1}\)</span>, and the coefficient <span class="math inline">\(\beta_{j-1}\)</span>, and want expand the Krylov subspace to find <span class="math inline">\(q_{j+1}\)</span> in a way that takes advantage of the three term recurrence. To do this we can expand the subspace by computing <span class="math inline">\(Aq_j\)</span> and then orthogonalizing <span class="math inline">\(Aq_j\)</span> against <span class="math inline">\(q_j\)</span> and <span class="math inline">\(q_{j-1}\)</span>. By the three term recurrence, <span class="math inline">\(Aq_j\)</span> will be orthogonal to <span class="math inline">\(q_i\)</span> for all <span class="math inline">\(i\leq j-2\)</span> so we do not need to explicitly orthogonalize against those vectors.</p>
<p>We orthogonalize, <span class="math display">\[\begin{align*}
\tilde{q}_{j+1} = Aq_j - \alpha_j q_j - \langle Aq_j, q_{j-1} \rangle q_{j-1}
, &amp;&amp; 
\alpha_{j} = \langle A q_j, q_j \rangle
\end{align*}\]</span> and finally normalize, <span class="math display">\[\begin{align*}
q_{j+1} = \tilde{q}_{j+1} / \beta_j
,&amp;&amp;
\beta_j = \|\tilde{q}_{j+1}\|
\end{align*}\]</span></p>
<p>Note that this is not the most “numerically stable” form of the algorithm, and care must be taken when implementing the Lanczos method in practice. We can improve stability slightly by using <span class="math inline">\(Aq_j - \beta_{j-1} q_{j-1}\)</span> instead of <span class="math inline">\(Aq_j\)</span> when finding a vector in the next Krylov subspace. This allows us to explicitly orthogonalize <span class="math inline">\(q_{j+1}\)</span> against both <span class="math inline">\(q_j\)</span> and <span class="math inline">\(q_{j-1}\)</span> rather than just <span class="math inline">\(q_j\)</span>. It also ensures that the tridiagonal matrix produces is symmetric in finite precision (since <span class="math inline">\(\langle Aq_j,q_{j-1}\rangle\)</span> may not be equal to <span class="math inline">\(\beta_j\)</span> in finite precision).</p>
<p><strong>Algorithm.</strong> (Lanczos) <span class="math display">\[\begin{align*}
&amp;\textbf{procedure}\text{ lanczos}( A,v ) 
\\[-.4em]&amp;~~~~\textbf{set } q_1 = v / \|v\|, \beta_0 = 0
\\[-.4em]&amp;~~~~\textbf{for } k=1,2,\ldots \textbf{:} 
\\[-.4em]&amp;~~~~~~~~\textbf{set } \tilde{q}_{k+1} = Aq_k - \beta_{k-1} q_{k-1}
\\[-.4em]&amp;~~~~~~~~\textbf{set } \alpha_k = \langle \tilde{q}_{k+1}, q_k \rangle
\\[-.4em]&amp;~~~~~~~~\textbf{set } \tilde{q}_{k+1} = \tilde{q}_{k+1} - \alpha_k q_{k}
\\[-.4em]&amp;~~~~~~~~\textbf{set } \beta_k = \| \tilde{q}_{k+1} \|
\\[-.4em]&amp;~~~~~~~~\textbf{set } q_{k+1} = \tilde{q}_{k+1} / \beta_k
\\[-.4em]&amp;~~~~~\textbf{end for}
\\[-.4em]&amp;\textbf{end procedure}
\end{align*}\]</span></p>
<!--
We can [implement](./lanczos.py) Lanczos iteration in numpy.
Here we assume that we only want to output the diagonals of the tridiagonal matrix $T$, and don't need any of the vectors (this would be useful if we wanted to compute the eigenvalues of $A$, but not the eigenvectors).

    def lanczos(A,q0,max_iter):
        alpha = np.zeros(max_iter)
        beta = np.zeros(max_iter)
        q_ = np.zeros(len(q0))
        q = q0/np.sqrt(q0@q0)

        for k in range(max_iter):
            qq = A@q-(beta[k-1]*q_ if k>0 else 0)
            alpha[k] = qq@q
            qq -= alpha[k]*q
            beta[k] = np.sqrt(qq@qq)
            q_ = np.copy(q)
            q = qq/beta[k]

    return alpha,beta
-->
<!--start_pdf_comment-->
<p>Next: <a href="./cg_derivation.html">A derivation of the conjugate gradient algorithm</a> <!--end_pdf_comment--></p>
<p class="footer">More about the conjugate gradient method can be found <a href="./">here</a>.</p>
</div>
</body>
</html>
