<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <title>A Derivation of the Conjugate Gradient Algorithm</title>
  <meta name="description" content="The Conjugate Conjugate algorithm is a widely used method for solving Ax=b when A is positive definite. While it’s simple to state the algorithm, understanding where it comes from is not always so clear.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
  <meta name="author" content="Tyler Chen" />
  <meta name="keywords" content="applied, math" />

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link href="../../tc.ico" rel="shortcut icon" >
  <link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
  <link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
  <link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />
  <link rel="stylesheet" href="../../font/vollkorn/stylesheet.css" type="text/css" charset="utf-8" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>

</head>
<body>
<div id="contentContainer">
<h1>A Derivation of the Conjugate Gradient Algorithm</h1>
<p class="author"><a href="https://chen.pw">Tyler Chen</a></p>
<p>There are many ways to view/derive the conjugate gradient algorithm.
I’ll derive the algorithm by directly minimizing by minimizing the <span class="math inline">\(A\)</span>-norm of the error over successive Krylov subspaces, <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>.
To me this is the most natural way of viewing the algorithm.
My hope is that the derivation here provides some motivation for where the algorithm comes from.
Of course, what I think is a good way to present the topic won’t match up exactly with every reader’s own preference, so I highly recommend looking through some other resources as well.
To me, this is of those topics where you have to go through the explanations a few times before you start to understand what is going on.</p>
<h2>Minimizing the error</h2>
<p>Now that we have that out of the way, let’s begin our derivation.
As stated above, at each step we will minimize the <span class="math inline">\(A\)</span>-norm of the error over successive Krylov subspaces generated by <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span>.
That is to say <span class="math inline">\(x_k\)</span> will be the point so that,
<span class="math display">\[\begin{align*}
\|e_k\|_A
:=\| x_k - x^* \|_A 
= \min_{x\in\mathcal{K}_k(A,b)} \| x - x^* \|_A
,&amp;&amp;
x^* = A^{-1}b
\end{align*}\]</span></p>
<p>Since we are minimizing with respect to the <span class="math inline">\(A\)</span>-norm, it will be useful to have an <span class="math inline">\(A\)</span>-orthonormal basis for <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>.
That is, a basis which is orthonormal in the <span class="math inline">\(A\)</span>-inner product.
For now, let’s just suppose we have such a basis, <span class="math inline">\(\{p_0,p_1,\ldots,p_{k-1}\}\)</span>, ahead of time.
Further on in the derivation we will explain a good way of coming up with this basis as we go.</p>
<p>Since <span class="math inline">\(x_k\in\mathcal{K}_k(A,b)\)</span> we can write <span class="math inline">\(x_k\)</span> as a linear combination of these basis vectors,
<span class="math display">\[
x_k = a_0 p_0 + a_1 p_1 + \cdots + a_{k-1} p_{k-1}
\]</span></p>
<p>Note that we have <span class="math inline">\(x_0 = 0\)</span> and <span class="math inline">\(e_k = x^* - x_k\)</span>.
Then,
<span class="math display">\[
e_k = e_0 - a_0p_0 - a_1 p_1 - \cdots - a_{k-1} p_{k-1}
\]</span></p>
<p>By definition, the coefficients for <span class="math inline">\(x_k\)</span> were chosen to minimize the <span class="math inline">\(A\)</span>-norm of the error, <span class="math inline">\(\|e_k\|_A\)</span>, over <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>.
Therefore, <span class="math inline">\(e_k\)</span> must have zero component in each of the directions <span class="math inline">\(\{ p_0,p_1,\ldots,p_{k-1} \}\)</span>, which is an <span class="math inline">\(A\)</span>-orthonormal basis for <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>.
In particular, that means that <span class="math inline">\(a_jp_j\)</span> cancels exactly with <span class="math inline">\(e_0\)</span> in the direction of <span class="math inline">\(p_j\)</span>, for all <span class="math inline">\(j\)</span>.</p>
<p>We now make the important observation that the coefficients depend only on <span class="math inline">\(e_0\)</span> and the <span class="math inline">\(p_i\)</span>, but not on <span class="math inline">\(k\)</span>.
This means that the coefficients <span class="math inline">\(a_0&#39;,a_1&#39;,\ldots,a_{k-2}&#39;\)</span> of <span class="math inline">\(x_{k-1}\)</span> were chosen in exactly the same way as the coefficients for <span class="math inline">\(x_k\)</span>, so <span class="math inline">\(a_0=a_0&#39;, a_1=a_1&#39;, \ldots, a_{k-2}=a_{k-2}&#39;\)</span>.</p>
<p>We can then write,
<span class="math display">\[
x_k = x_{k-1} + a_{k-1} p_{k-1}
\]</span>
and
<span class="math display">\[
e_k = e_{k-1} - a_{k-1} p_{k-1}
\]</span></p>
<p>Now that we have explicitly written <span class="math inline">\(x_k\)</span> in terms of an update to <span class="math inline">\(x_{k-1}\)</span> this is starting to look like an iterative method!</p>
<p>Let’s compute an explicit representation of the coefficient <span class="math inline">\(a_{k-1}\)</span>.
As previously noted, since we have chosen <span class="math inline">\(x_k\)</span> to minimize <span class="math inline">\(\|e_k\|_A\)</span> over <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>, the component of <span class="math inline">\(e_k\)</span> in each of the directions <span class="math inline">\(p_0,p_1,\ldots,p_{k-1}\)</span> must be zero.
That is, <span class="math inline">\(\langle e_k , p_j \rangle = 0\)</span> for all <span class="math inline">\(i=0,1,\ldots, k-1\)</span>. Substituting our expression for <span class="math inline">\(e_k\)</span> we find,
<span class="math display">\[
0 = \langle e_k , p_{k-1} \rangle_A
= \langle e_{k-1}, p_{k-1} \rangle - a_{k-1} \langle p_{k-1} , p_{k-1} \rangle_A
\]</span></p>
<p>Thus,
<span class="math display">\[
a_{k-1} 
= \frac{\langle e_{k-1}, p_{k-1} \rangle_A}{\langle p_{k-1},p_{k-1} \rangle_A} 
\]</span></p>
<p>This expression might look like a bit of a roadbock, since if we knew the error <span class="math inline">\(e_k = x^* - x_k\)</span> then we would know how to obtain the solution from our guess!
However, we have been working with the <span class="math inline">\(A\)</span>-inner product so we can write,
<span class="math display">\[
Ae_{k-1} = A(x^* - x_{k-1}) = b - Ax_{k-1} = r_{k-1}
\]</span>
Therefore, <span class="math inline">\(a_{k-1}\)</span> can be written as,
<span class="math display">\[
a_{k-1}
= \frac{\langle r_{k-1}, p_{k-1} \rangle}{\langle p_{k-1},A p_{k-1} \rangle} 
\]</span></p>
<h2>Finding the Search Directions</h2>
<p>At this point we are almost done.
The last thing to do is understand how to construct the basis <span class="math inline">\(\{p_0,p_1,\ldots,p_k\}\)</span> as we go.
Since <span class="math inline">\(A\)</span> is symmetric we know there is a three term recurrence which gives an orthonormal basis. However, the basis <span class="math inline">\(\{p_0,p_1,\ldots,p_k\}\)</span> is <span class="math inline">\(A\)</span>-orthogonal.
Even so, looking for a short recurrence sounds promising.</p>
<p>Since <span class="math inline">\(r_k = b-Ax_k\)</span> and <span class="math inline">\(x_k\in\mathcal{K}_k(A,b)\)</span>, then <span class="math inline">\(r_k \in \mathcal{K}_{k+1}(A,b)\)</span>.
Thus, we can obtain <span class="math inline">\(p_k\)</span> by <span class="math inline">\(A\)</span>-orthogonalizing <span class="math inline">\(r_k\)</span> against <span class="math inline">\(\{p_0,p_1,\ldots,p_{k-1}\}\)</span>.</p>
<p>Recall that <span class="math inline">\(e_k\)</span> is <span class="math inline">\(A\)</span>-orthogonal to <span class="math inline">\(\mathcal{K}_k(A,b)\)</span>.
That is, for <span class="math inline">\(j\leq k-1\)</span>,
<span class="math display">\[
\langle e_k, A^j b \rangle_A = 0
\]</span></p>
<p>Therefore, noting that <span class="math inline">\(Ae_k = r_k\)</span>, for <span class="math inline">\(j\leq k-2\)</span>,
<span class="math display">\[
\langle r_k, A^j b \rangle_A = 0
\]</span></p>
<p>That is, <span class="math inline">\(r_k\)</span> is <span class="math inline">\(A\)</span>-orthogonal to <span class="math inline">\(\mathcal{K}_{k-1}(A,b)\)</span>.
In particular, this means that, for <span class="math inline">\(j\leq k-2\)</span>,
<span class="math display">\[
\langle r_k, p_j \rangle_A = 0
\]</span></p>
<p>That means that to obtain <span class="math inline">\(p_k\)</span> we really only need to <span class="math inline">\(A\)</span>-orthogonalize <span class="math inline">\(r_k\)</span> against <span class="math inline">\(p_{k-1}\)</span> instead of all the previous <span class="math inline">\(p_i\)</span>! That is,
<span class="math display">\[\begin{align*}
p_k = r_k + b_k p_{k-1}
,&amp;&amp;
b_k = - \frac{\langle r_k, p_{k-1} \rangle_A}{\langle p_{k-1}, p_{k-1} \rangle_A}
\end{align*}\]</span></p>
<p>The immediate consequence is that we do not need to save the entire basis <span class="math inline">\(\{p_0,p_1,\ldots,p_{k-1}\}\)</span>, but instead can just keep <span class="math inline">\(x_k\)</span>,<span class="math inline">\(r_k\)</span>, and <span class="math inline">\(p_{k-1}\)</span>.
This is perhaps somewhat unsurprising give then we have seen that when <span class="math inline">\(A\)</span> is symmetric we have a three term <a href="./arnoldi_lanczos.html#the-lanczos-algorithm">Lanczos recurrence</a>.
In fact, it turns out that the relationship between CG and Lanczos is quite fundamental, and that <a href="./cg_lanczos.html">CG is essentially doing the Lanczos algorithm</a>.</p>
<h2>Putting it all together</h2>
<p>We are now essentially done! In practice, people generally use the following equivalent (but more numerically stable) formulas for <span class="math inline">\(a_{k-1}\)</span> and <span class="math inline">\(b_k\)</span>,
<span class="math display">\[\begin{align*}
a_{k-1} = \frac{\langle r_{k-1},r_{k-1}\rangle}{\langle p_{k-1},Ap_{k-1}\rangle}
,&amp;&amp;
b_k = \frac{\langle r_k,r_k\rangle}{\langle r_{k-1},r_{k-1}\rangle}
\end{align*}\]</span></p>
<p>The first people to discover this algorithm Magnus Hestenes and Eduard Stiefel who independently developed it around 1952. As such, the standard implementation is attributed to them.
Pseudocode is presented below.</p>
<p><strong>Algorithm.</strong> (Hestenes and Stiefel conjugate gradient)
<span class="math display">\[\begin{align*}
&amp;\textbf{procedure}\text{ HSCG}( A,b,x_0 ) 
\\[-.4em]&amp;~~~~r_0 = b-Ax_0, \nu_0 = \langle r_0,r_0 \rangle, p_0 = r_0, s_0 = Ar_0, 
\\[-.4em]&amp;~~~~a_0 = \nu_0 / \langle p_0,s_0 \rangle
\\[-.4em]&amp;~~~~\textbf{for } k=1,2,\ldots \textbf{:} 
\\[-.4em]&amp;~~~~~~~~x_k = x_{k-1} + a_{k-1} p_{k-1} 
\\[-.4em]&amp;~~~~~~~~r_k = r_{k-1} - a_{k-1} p_{k-1} 
\\[-.4em]&amp;~~~~~~~~\nu_{k} = \langle r_k,r_k \rangle, \textbf{ and } b_k = \nu_k / \nu_{k-1}
\\[-.4em]&amp;~~~~~~~~p_k = r_k + b_k p_{k-1}
\\[-.4em]&amp;~~~~~~~~s_k = A p_k
\\[-.4em]&amp;~~~~~~~~\mu_k = \langle p_k,s_k \rangle, \textbf{ and } a_k = \nu_k / \mu_k
\\[-.4em]&amp;~~~~~\textbf{end for}
\\[-.4em]&amp;\textbf{end procedure}
\end{align*}\]</span></p>
<!--
\begin{align*}
&\textbf{procedure}\text{ HSCG}( A,b,x_0 ) 
\\[-.4em]&~~~~\textbf{set } r_0 = b-Ax_0, \nu_0 = \langle r_0,r_0 \rangle, p_0 = r_0, s_0 = Ar_0, 
\\[-.4em]&~~~~\phantom{\textbf{set }}a_0 = \nu_0 / \langle p_0,s_0 \rangle
\\[-.4em]&~~~~\textbf{for } k=1,2,\ldots \textbf{:} 
\\[-.4em]&~~~~~~~~\textbf{set } x_k = x_{k-1} + a_{k-1} p_{k-1} 
\\[-.4em]&~~~~~~~~\phantom{\textbf{set }} r_k = r_{k-1} - a_{k-1} p_{k-1} 
\\[-.4em]&~~~~~~~~\textbf{set } \nu_{k} = \langle r_k,r_k \rangle, \textbf{ and } b_k = \nu_k / \nu_{k-1}
\\[-.4em]&~~~~~~~~\textbf{set }p_k = r_k + b_k p_{k-1}
\\[-.4em]&~~~~~~~~\textbf{set }s_k = A p_k
\\[-.4em]&~~~~~~~~\textbf{set }\mu_k = \langle p_k,s_k \rangle, \textbf{ and } a_k = \nu_k / \mu_k
\\[-.4em]&~~~~~\textbf{end for}
\\[-.4em]&\textbf{end procedure}
\end{align*}
-->
<!--
This can be easily [implemented](./cg.py) in numpy.
Note that we use $f$ for the right hand side vector to avoid conflict with the coefficient $b$.

    def cg(A,f,max_iter):
        x = np.zeros(len(f)); r = np.copy(f); p = np.copy(r); s=A@p
        nu = r @ r; a = nu/(p@s); b = 0
        for k in range(1,max_iter):
            x += a*p
            r -= a*s

            nu_ = nu
            nu = r@r
            b = nu/nu_

            p = r + b*p
            s = A@p

            a = nu/(p@s)

        return x
-->
<!--start_pdf_comment-->
<p>Next: <a href="./cg_lanczos.html">conjugate gradient is Lanczos in Disguise</a>
<!--end_pdf_comment--></p>
<p class="footer">
More about the conjugate gradient method can be found <a href="./">here</a>.
</p>
</div>
</body>
</html>
