<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <title>Optimal low-memory rational matrix function approximation</title>
  <meta name="description" content="We provide some new error bounds for the Lanczos methods for computing matrix functinos.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
  <meta name="author" content="Tyler Chen" />

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <link href="../../tc.ico" rel="shortcut icon" >
  <link href="../../css/main.css" rel="stylesheet" type="text/css" media="screen" />
  <link href="../../css/print.css" rel="stylesheet" type="text/css" media="print"/>
  <link rel="stylesheet" href="../../font/lato/stylesheet.css" type="text/css" charset="utf-8" />
  <link rel="stylesheet" href="../../font/vollkorn/stylesheet.css" type="text/css" charset="utf-8" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>

</head>
<body>
<div id="contentContainer">
<h1>Optimal low-memory rational matrix function approximation</h1>
<p class="author"><a href="https://chen.pw">Tyler Chen</a></p>
<p>This is a companion piece to the publication:</p>
<p><pre>@misc{lowmem_rational_opt,
    title={Low-memory Krylov subspace methods for optimal rational matrix function approximation},
    author={Tyler Chen and Anne Greenbaum and Cameron Musco and Christopher Musco},
    year={2022},
    eprint={2202.11251},
    archivePrefix={arXiv},
    primaryClass={math.NA}
}</pre></p>
<h2>Why should I care?</h2>
<p><a href="./lanczos_function_CIF.html">Matrix functions</a> have a myriad of applications in nearly every field of computational science.
Among the most powerful algorithms for computing the product of a matrix function <span class="math inline">\(f[\mathbf{A}]\)</span> with a vector <span class="math inline">\(\mathbf{b}\)</span> are Krylov subspace methods such as the Lanczos method for matrix function approximation (Lanczos-FA).
These algorithms accecss <span class="math inline">\(\mathbf{A}\)</span> only through matrix products <span class="math inline">\(\mathbf{v} \mapsto \mathbf{A}\mathbf{v}\)</span>, and are therefore well suited for situations in which <span class="math inline">\(\mathbf{A}\)</span> is too large to store in fast memory.
However, for general matrix functions, the amount of storage required often grows linearly with the number of iterations, resulting in a computational bottleneck.</p>
<h2>Introduction</h2>
<p>If <span class="math inline">\(f(x) = 1/x\)</span>, then <span class="math inline">\(f[\mathbf{A}]\mathbf{b} = \mathbb{A}^{-1} \mathbf{b}\)</span> corresponds to the solution of the linear system of equations <span class="math inline">\(\mathbf{A} \mathbf{x} = \mathbf{b}\)</span>.
In this case, algorithms such as the conjugate gradient algorithm and the minimum residual algorithm can approximate <span class="math inline">\(\mathbf{A}^{-1} \mathbf{b}\)</span> using an amount of storage <em>independent</em> of the number of iterations taken.
For general functions, a range of techniques such as restarting <a href="">[]</a> or two-pass Lanczos <a href="">[]</a> have been developed which also do not require memory increasing with the number of iterations.
However, such methods have slower convergence or require more matrix-vector products than Lanczos-FA.</p>
<h2>Contributions of this paper</h2>
<p>In this paper, we first describe a mathematical algorithms which outputs the <em>optimal</em> approximation to a rational matrix function.
Here optimal is with respect to a certain norm which depends on the rational function in question.
Both the conjugate gradient algorithm and the minimum residual algorithms are obtained as special cases of our optimal approximation.
We then give low-memory implementations of our optimal algorithm and of Lanczos-FA which do not require more storage as more iterations are run.<br />
Finally, we use this optimal approximation to derive (non-optimal) approximations to other matrix functions such the matrix-sign function.</p>
<p class="footer">
The rest of my publications can be found <a href="./../">here</a>.
</p>
</div>
</body>
</html>
