<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Tyler Chen</title>
<meta name="description" content="I'm Tyler Chen, applied math PhD student at the University of Washington. Find out more about my research, teaching, and educational beliefs, and then get in contact with me.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../tc.ico" rel="shortcut icon" >
<link href="../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../font/lato/stylesheet.css" type="text/css" charset="utf-8" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>

<div id="contentContainer">
<h1> Krylov Subspace Methods
</h1>
<p class="authors"> Tyler Chen
</p>
<p>A pdf version of this page can be found <a href="./krylov_methods.pdf">here</a></p>
<h3 id="introduction">Introduction</h3>

<!--eventually may want to move this to an introduction to linear systems-->

<p>Solving a linear system of equations <span class="math inline"><em>A</em><em>x</em> = <em>b</em></span> is one of the most important tasks in modern science. Applications such as weather forecasting, medical imaging, and training neural nets all require repeatedly solving linear systems.</p>

<p>Loosely speaking, methods for linear systems can be separated into two categories: direct methods and iterative methods. Direct methods such as Gaussian elimination manipulate the entries of the matrix <span class="math inline"><em>A</em></span> in order to compute the solution <span class="math inline"><em>x</em> = <em>A</em><sup>−1</sup><em>b</em></span>. On the other hand, iterative methods generate a sequence <span class="math inline"><em>x</em><sub>0</sub>, <em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …</span> of approximations to the true solution <span class="math inline"><em>x</em> = <em>A</em><sup>−1</sup><em>b</em></span>, where hopefully each iterate is a better approximation to the true solution.</p>

<p>At first glance, it may seem that direct methods are better. After all, after a known number of steps you get the exact solution. In many cases this is true, especially when the matrix <span class="math inline"><em>A</em></span> is dense and relatively small. The main drawback to direct methods is that they are not able to easily take advantage of sparsity. That means that even if <span class="math inline"><em>A</em></span> has some nice structure and a lot of entries are zero, direct methods will take the same amount of time and storage to compute the solution as if <span class="math inline"><em>A</em></span> were dense. This is where iterative methods come in. Often times iterative methods require only that the product <span class="math inline"><em>x</em> ↦ <em>A</em><em>x</em></span> be able to be computed. If <span class="math inline"><em>A</em></span> is sparse the product can be done cheaply, and if <span class="math inline"><em>A</em></span> has some known structure, a you might not even need to construct <span class="math inline"><em>A</em></span>. Such methods are aptly called &quot;matrix free&quot;.</p>

<p>The rest of this piece gives an introduction to Krylov subspace methods, a common type of iterative method. My intention is not to provide a rigorous explanation of the topic, but rather to provide some (hopefully useful) intuition about where these methods come from and why they are useful in practice. I assume some linear algebra background (roughly at the level of a first undergrad course in linear algebra).</p>

<p>If you are a bit rusty on your linear algebra I suggest taking a look at [some resource]. If you want a more rigorous introduction to iterative methods I suggest Anne Greenbaum's <a href="https://epubs.siam.org/doi/book/10.1137/1.9781611970937?mobileUi=0u">book</a>. For a overview of modern analysi of the Lanczos and Conjugate Gradient methods I suggest Gerard Meurant and Zdenek Strakos's <a href="https://www.karlin.mff.cuni.cz/~strakos/download/2006_MeSt.pdf">report</a>.</p>

<h3 id="krylov-subspaces">Krylov subspaces</h3>

<p>sequence of subspaces contained in the previous</p>

<ol style="list-style-type: decimal">

<li>easy to construct</li>

<li>eventually should contain solution</li>

<li>easy to optimize (some quantity) over</li>

</ol>

<p>The <span class="math inline"><em>k</em></span>-th Krylov subspace generated by a square matrix <span class="math inline"><em>A</em></span> and a vector <span class="math inline"><em>v</em></span> is defined to be, <br /><span class="math display">𝒦<sub><em>k</em></sub>(<em>A</em>, <em>v</em>)=span{<em>v</em>, <em>A</em><em>v</em>, …, <em>A</em><sup><em>k</em> − 1</sup><em>v</em>}</span><br /></p>

<p>These subspaces are relatively easy to construct because we can get them by repeatedly applying <span class="math inline"><em>A</em></span> to <span class="math inline"><em>v</em></span>. On the other hand, it's not so clear that they will ever contain our solution <span class="math inline"><em>x</em> = <em>A</em><sup>−1</sup><em>b</em></span> or that it is easy to optimize over them.</p>

<p>We first show that <span class="math inline">𝒦<sub><em>k</em></sub>(<em>A</em>, <em>b</em>)</span> will eventually contain our solution by the time <span class="math inline"><em>k</em> = <em>n</em></span>. While this result comes about naturally later from our description of some algorithms I think it is useful to immediately relate polynomials with Krylov subspace methods, as the two are intimately related.</p>

<p>Suppose <span class="math inline"><em>A</em></span> has characteristic polynomial,<br /><span class="math display"><em>p</em><sub><em>A</em></sub>(<em>t</em>)=det(<em>t</em><em>I</em> − <em>A</em>)=<em>c</em><sub>0</sub> + <em>c</em><sub>1</sub><em>t</em> + ⋯ + <em>c</em><sub><em>n</em> − 1</sub><em>t</em><sup><em>n</em> − 1</sup> + <em>t</em><sup><em>n</em></sup></span><br /> It turns out that <span class="math inline"><em>c</em><sub>0</sub> = ( − 1)<sup><em>n</em></sup>det(<em>A</em>)</span> so that <span class="math inline"><em>c</em><sub>0</sub></span> is nonzero if <span class="math inline"><em>A</em></span> is invertible.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem">Cayley-Hamilton Theorem</a> states that a matrix satisfies its own <a href="https://en.wikipedia.org/wiki/Characteristic_polynomial#Characteristic_equation">characteristic polynomial</a>. This means, <br /><span class="math display">0 = <em>p</em><sub><em>A</em></sub>(<em>A</em>)=<em>c</em><sub>0</sub><em>I</em> + <em>c</em><sub>1</sub><em>A</em> + ⋯<em>c</em><sub><em>n</em> + 1</sub><em>A</em><sup><em>n</em> − 1</sup> + <em>A</em><sup><em>n</em></sup></span><br /></p>

<p>Moving the identity term to the left and dividing by <span class="math inline">−<em>c</em><sub>0</sub></span> we can write, <br /><span class="math display"><em>A</em><sup>−1</sup> = −(<em>c</em><sub>1</sub>/<em>c</em><sub>0</sub>)<em>I</em> − (<em>c</em><sub>2</sub>/<em>c</em><sub>0</sub>)<em>A</em> + ⋯ − (1/<em>c</em><sub>0</sub>)<em>A</em><sup><em>n</em> − 1</sup></span><br /></p>

<p>This says that <span class="math inline"><em>A</em><sup>−1</sup></span> can be written as a polynomial in <span class="math inline"><em>A</em></span>! In particular,<br />

<br /><span class="math display"><em>x</em> = <em>A</em><sup>−1</sup><em>b</em> = −(<em>c</em><sub>1</sub>/<em>c</em><sub>0</sub>)<em>b</em> − (<em>c</em><sub>2</sub>/<em>c</em><sub>0</sub>)<em>A</em><em>b</em> + ⋯ − (1/<em>c</em><sub>0</sub>)<em>A</em><sup><em>n</em> − 1</sup><em>b</em></span><br /></p>

<p>That means that the solution to <span class="math inline"><em>A</em><em>x</em> = <em>b</em></span> is a linear combination of <span class="math inline"><em>b</em>, <em>A</em><em>b</em>, <em>A</em><sup>2</sup><em>b</em>, …, <em>A</em><sup><em>n</em> − 1</sup><em>b</em></span>. This observation is the motivation behind Krylov subspace methods. In particular, Krylov subspace methods work by building low degree polynomial approximations to <span class="math inline"><em>A</em><sup>−1</sup></span> using powers of <span class="math inline"><em>A</em></span>.</p>

<p>We then see that <span class="math inline"><em>x</em> = <em>A</em><sup>−1</sup><em>b</em> ∈ 𝒦<sub><em>n</em></sub>(<em>A</em>, <em>b</em>)</span>.</p>

<h3 id="arnoldi-algorithm">Arnoldi Algorithm</h3>

<p>The Arnoldi algorithm for computing an orthonormal basis for Krylov subspaces is at the core of most Krylov subspace methods. Essentially, the Arnoldi algorithm is the Gram-Schmidt procedure applied to the vectors <span class="math inline"><em>v</em>, <em>A</em><em>v</em>, <em>A</em><sup>2</sup><em>v</em>, <em>A</em><sup>3</sup><em>v</em>, …</span> in a clever way.</p>

<p>Recall that given a set of vectors <span class="math inline"><em>v</em><sub>0</sub>, <em>v</em><sub>1</sub>, …, <em>v</em><sub><em>k</em></sub></span> the Gram-Schmidt procedure computes an orthonormal basis <span class="math inline"><em>q</em><sub>0</sub>, <em>q</em><sub>1</sub>, …, <em>q</em><sub><em>k</em></sub></span> so that for all <span class="math inline"><em>j</em> ≤ <em>k</em></span>, <br /><span class="math display">span{<em>v</em><sub>0</sub>, …, <em>v</em><sub><em>j</em></sub>}=span{<em>q</em><sub>0</sub>, …, <em>q</em><sub><em>j</em></sub>}</span><br /></p>

<p>The trick behind the Arnoldi algorithm is the fact that you do not need to construct the whole set <span class="math inline"><em>v</em>, <em>A</em><em>v</em>, <em>A</em><sup>2</sup><em>v</em>, …</span> ahead of time. Instead, you can compute <span class="math inline"><em>A</em><em>q</em><sub><em>j</em></sub></span> in place of <span class="math inline"><em>A</em><sup><em>j</em> + 1</sup><em>v</em></span> once you have found an orthonormal basis <span class="math inline"><em>q</em><sub>0</sub>, <em>q</em><sub>1</sub>, …, <em>q</em><sub><em>j</em></sub></span> spanning <span class="math inline"><em>v</em>, <em>A</em><em>v</em>, …, <em>A</em><sup><em>j</em></sup><em>v</em></span>.</p>

<p>If we assume that <span class="math inline">span{<em>v</em>, <em>A</em><em>v</em>, …<em>A</em><sup><em>j</em></sup><em>v</em>}=span{<em>q</em><sub>0</sub>, …, <em>q</em><sub><em>j</em></sub>}</span> then <span class="math inline"><em>q</em><sub><em>j</em></sub></span> can be written as a linear combination of <span class="math inline"><em>v</em>, <em>A</em><em>v</em>, …, <em>A</em><sup><em>j</em></sup><em>v</em></span>. Therefore, <span class="math inline"><em>A</em><em>q</em><sub><em>j</em></sub></span> will be a linear combination of <span class="math inline"><em>A</em><em>v</em>, <em>A</em><sup>2</sup><em>v</em>, …, <em>A</em><sup><em>j</em> + 1</sup><em>v</em></span>. In particular, this means that <span class="math inline">span{<em>q</em><sub>0</sub>, …, <em>q</em><sub><em>j</em></sub>, <em>A</em><em>q</em><sub><em>j</em></sub>}=span{<em>v</em>, <em>A</em><em>v</em>, …, <em>A</em><sup><em>j</em> + 1</sup><em>v</em>}</span>. Therefore, we will get exactly the same set of vectors by applying Gram-Schmidt to <span class="math inline">{<em>v</em>, <em>A</em><em>v</em>, …, <em>A</em><sup><em>k</em></sup><em>v</em>}</span> as if we compute <span class="math inline"><em>A</em><em>q</em><sub><em>j</em></sub></span> once we have computing <span class="math inline"><em>q</em><sub><em>j</em></sub></span>.</p>

<p>Since we obtain <span class="math inline"><em>q</em><sub><em>j</em> + 1</sub></span> by orthogonalizing <span class="math inline"><em>A</em><em>q</em><sub><em>j</em></sub></span> against <span class="math inline">{<em>q</em><sub>0</sub>, <em>q</em><sub>1</sub>, …, <em>q</em><sub><em>j</em></sub>}</span> then <span class="math inline"><em>q</em><sub><em>j</em> + 1</sub></span> is in the span of these vectors. That means we can can some <span class="math inline"><em>c</em><sub><em>i</em></sub></span> so that, <br /><span class="math display"><em>q</em><sub><em>j</em> + 1</sub> = <em>c</em><sub>0</sub><em>q</em><sub>0</sub> + <em>c</em><sub>1</sub><em>q</em><sub>1</sub> + ⋯ + <em>c</em><sub><em>j</em></sub><em>q</em><sub><em>j</em></sub> + <em>c</em><sub><em>j</em> + 1</sub><em>A</em><em>q</em><sub><em>j</em></sub></span><br /></p>

<p>If we rewrite using new scalars <span class="math inline"><em>d</em><sub><em>i</em></sub></span> we have, <br /><span class="math display"><em>A</em><em>q</em><sub><em>j</em></sub> = <em>d</em><sub>0</sub><em>q</em><sub>0</sub> + <em>d</em><sub>1</sub><em>q</em><sub>1</sub> + ⋯ + <em>d</em><sub><em>j</em> + 1</sub><em>q</em><sub><em>j</em> + 1</sub></span><br /></p>

<p>This can be written in matrix form as, <br /><span class="math display"><em>A</em><em>Q</em> = <em>Q</em><em>H</em></span><br /> where <span class="math inline"><em>H</em></span> is &quot;upper Hessenburg&quot; (like upper triangular but the first subdiagonal also has nonzero entries). In fact, while we have not showed it, the entries of <span class="math inline"><em>H</em></span> come directly from the Arnoldi algorithm (just like how the entries of <span class="math inline"><em>R</em></span> in a QR factorization can be obtained from Gram Schmidt).</p>

<h3 id="conjugate-gradient">Conjugate Gradient</h3>

<p>When <span class="math inline"><em>A</em></span> is Hermetian, then <span class="math inline"><em>Q</em><sup>*</sup><em>A</em><em>Q</em> = <em>H</em></span> is also Hermetian. Since <span class="math inline"><em>H</em></span> is upper Hessenburg and Hermitian, it must be tridiagonal! This means that the <span class="math inline"><em>q</em><sub><em>j</em></sub></span> satisfy a three term recurrence, <br /><span class="math display"><em>A</em><em>q</em><sub><em>j</em></sub> = <em>β</em><sub><em>j</em> − 1</sub><em>q</em><sub><em>j</em> − 1</sub> + <em>α</em><sub><em>j</em></sub><em>q</em><sub><em>j</em></sub> + <em>β</em><sub><em>j</em></sub><em>q</em><sub><em>j</em> + 1</sub></span><br /> where <span class="math inline"><em>α</em><sub>1</sub>, …, <em>α</em><sub><em>n</em></sub></span> are the diagonal entries of <span class="math inline"><em>T</em></span> and <span class="math inline"><em>β</em><sub>1</sub>, …, <em>β</em><sub><em>n</em> − 1</sub></span> are the off diagonal entries of <span class="math inline"><em>T</em></span>.</p>

<p>When <span class="math inline"><em>A</em></span> is symmetric positive definite (all eigenvalues are positive) then a common method for solving <span class="math inline"><em>A</em><em>x</em> = <em>b</em></span> is Conjugate Gradient.</p>

<p class="footer">The rest of my research can be found <a href="./">here</a>.</p>
</div>
</body>
</html>
