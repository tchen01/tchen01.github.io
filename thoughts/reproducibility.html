<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Tyler Chen</title>
<meta name="description" content="I'm Tyler Chen, applied math PhD student at the University of Washington. Find out more about my research, teaching, and educational beliefs, and then get in contact with me.">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width user-scalable=no">

<link href="../tc.ico" rel="shortcut icon" >
<link href="../css/main.css" rel="stylesheet" type="text/css" media="screen" />
<link href="../css/print.css" rel="stylesheet" type="text/css" media="print"/>
<link rel="stylesheet" href="../font/lato/stylesheet.css" type="text/css" charset="utf-8" />

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-50592837-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-50592837-1');
</script>
</head>

<body>
<div id="contentContainer">
    <h1>Reproducibility, Inclusivity, and Open Science</h1>
	<p>Note that I'm still in the process of editing this piece.</p>
    <p>Science is built on the idea that if two people run the same experiment, they should get the same results. 

There are many fields where it is very easy for researchers to make the costs (time, money, etc) of reproducing their experiments low. I believe, that in these cases, it is important that scientists make a good faith effort to do so, and to do so in a way which is inclusive of the broader scientific community.</p> 
    <p>I'll discuss the issue in the context of computational science, but most of what I'm saying applies to other fields as well.</p>
        
    <!--h3>A simple example</h3>
    <p>Imagine the task of adding 4 numbers <span class="math"><em>a</em>, <em>b</em>, <em>c</em>, <em>d</em></span>. There are lots of ways (orders) to do this. The natural way is <span class="math">((<em>a</em> + <em>b</em>) + <em>c</em>) + <em>d</em></span>, but another way to do it would be <span class="math">(<em>a</em> + <em>b</em>) + (<em>c</em> + <em>d</em>)</span>. The advantage of the second way is that you can compute <span class="math"><em>a</em> + <em>b</em></span> and <span class="math"><em>c</em> + <em>d</em></span> at the same time (in parallel). More general, if you and a friend had to compute the sum of a million numbers, you would split them into two groups and work on each half at the same time. For really big problems (lots of numbers), the only reasonable way to solve them is to parallelize your algorithms.</p>
    <p>Let's consider an example. Suppose the computer can only store two significant digits of a number. So the best representation of <span class="math">1.234</span> is <span class="math">1.2</span> and the best representation of <span class="math">0.658</span> is <span class="math">0.66</span>. We also have the property that when adding two floating point numbers, the output is the closest rounded number to the exact sum. So on the toy computer adding <span class="math">1.2</span> and <span class="math">0.66</span> gives $1.9</span>, since the exact sum is $1.86</span> but we can only keep 2 digits. Perhaps unsurprisingly, these properties means that the order we add numbers matters; i.e. floating point addition is not associative.</p>
    <p>Suppose <span class="math"><em>a</em> = 1.234, <em>b</em> = 0.658, <em>c</em> = 0.246, <em>d</em> = 0.112</span>. To input these to our toy computer we round them to <span class="math">1.2, 0.66, 0.25</span>, and <span class="math">0.11</span>. If we add them in order the computation would look like this: <span class="math">1.2 + 0.66 → 1.9</span>, <span class="math">1.9 + 0.25 → 2.2</span>, <span class="math">2.2 + 0.11 → 2.3</span>. On the other hand, if we add them in the second way, we would compute <span class="math">1.2 + 0.66 → 1.9</span> and <span class="math">0.25 + 0.11 → 0.36</span> first, and then combine them as <span class="math">1.9 + 0.36 → 1.4</span>. Note that the exact sum was <span class="math">2.25</span>.</p>
    <p>Imagine the task of adding up a hundred numbers. If you are alone, you might just keep a running sum. That is, add the second number to the first, the third to that sum, and so on. Suppose one of your friends joins you. Then it would make sense to distribute the work. You can add half the numbers, and your friend can add the other half, and then once you're both done, you can add up the two partial sums. This is an example of parallelizing the task at hand, and for a lot of real world problems, it is the only way to solve them in a reasonable amount of time. <p>
    <p>Mathematically, these two ways of adding the numbers are equivalent. That is, given the same input (which numbers you're adding), you expect exactly the same output (the sum). However, computers are finite and discrete, so they cannot represent every number exactly. Instead, they round numbers to one they can represent. This means that when you add two numbers on a computer, the output is not always exactly right.</p> 
    <p>Let's consider an example. Suppose the computer can only store two significant digits of a number. So the best representation of 1.234 is 1.2, and the best representation of 0.658 is 0.66. We also have the property that when adding two floating point numbers, the output is the closest rounded number to the exact sum. So on the toy computer adding 1.2 and 0.66 gives 1.9, since the exact sum is$1.86 but we can only keep 2 digits. Perhaps unsurprisingly, these properties means that the order we add numbers matters; i.e. floating point addition is not associative.</p>
    <p>Suppose we want to add 1.234, 0.658, 0.246, and 0.112 on our toy computer. To input these to our computer we round the numbers to 1.2, 0.66, 0.25, and 0.11 respectively. If we add them in order we first add 1.2 and 0.66 to get 1.86 which is rounded to 1.9. We then add 0.25 to 1.9 to get 2.15 which is rounded to 2.2. Finally, we add 2.2 and 0.11 to get 2.31 which is rounded to 2.3. What happens if we first group the numbers as 1.2, 0.66 and 0.25 and 0.11 and then add them? We already saw the first group becomes 1.9. The second group sums to 0.36. Combining the two groups gives 2.36 which rounds to 2.4 instead of 2.3 like the first method.
	<p>This illustrates that even though our two adding algorithms are mathematically equivalent, they can give different results on a computer. Now, to be fair, computers can store a lot more than 2 digits, and so in practice, both sums would probably have come out exactly the same and matched up with the exact answer. However, many algorithms and applications require millions and millions of additions (and other basic operations), so errors do show up. Errors made at one point propagate through calculations, and so sometimes the answer the computer gives is nowhere near correct, even if the algorithm is mathematically sound. The field of numerical analysis is about understanding when this happens, and when it doesn't.</p-->
        
    <p>A lot of papers relating to my research go something like this: "(~ task ~) is important for (~ grant keywords ~) reasons. Recently, because of (~ big data ~), there has been interest in fast algorithms for (~ task ~). Other papers have tried using (~ some method ~) quickly, however there are limitations with (~ some method ~). We propose an alternative to (~ some method ~) to address the limitations of (~ past paper ~)". A mathematical description of the new algorithm is presented, and then generally some numerical experiments which demonstrate that the new method works, and is possibly better than the old methods are included.</p>
    <p>Often, in order to understand the paper better, I will try to replicate the results of the paper. Based on the theoretical description algorithm, I can implement it myself and try to reproduce their results. However, in practice, a lot of pieces of the algorithm may be dependent on the exact setup of the machine the code is run on. For example, if the original authors used BLACKBOX™ to write their code, including built in functions for lower level tasks such as matrix multiplication, there is almost no chance that I will end up with an identical implementation.</p>

    <h3>So, what's the problem?</h3>
    <p>The results of the paper are technically reproducible. If I run the exact code the authors used on the right version of BLACKBOX™ I will get the same output. However, even assuming that the authors posted their code for others to verify (which is relatively uncommon), there are still reproducibility issues. First, there is no way of knowing the exact algorithms used in the BLACKBOX™ builtin functions. Second, since BLACKBOX™ is needed to reproduce the experiment, only those with access to BLACKBOX™ are able to take part in the verification process. This immediately excludes entire demographics who may not be able to justify the purchase of BLACKBOX™; i.e. high school students (especially those from low income areas), people in developing countries, hobbyists and amateur scientists, etc.</p>
    <p>The first issue has been discussed many times (see any MATLAB vs. Python argument thread). Generally, paid software such as MATLAB and Mathematica are well tested, and for the most part have well implemented algorithms. Even so, trust is required since nobody outside of those companies has access to their source code. In my opinion, using this type of software is only detrimental to the scientific process, especially given that there are a wide range of open source alternatives.</p>  
    <p>The second issue is less talked about, and extends far beyond reproducibility. While there are many cases where using proprietary software may be unavoidable, there are a huge number of cases where these programs are used only because of familiarity or personal comfort. The fact that many research institutions provide free access to paid software only perpetrates the problem, since researchers can easily forget that not everyone has the same level of access to these programs as they do. This has the effect of excluding less privileged persons from the scientific process and furthering academia's (well deserved) reputation as an "ivory tower" inaccessible to those without sufficient resources.</p>
	
	<h3>Why should academics in stem care?</h3>
	<p>It's reasonable to ask is why should researchers care about any of this. Since most people in academia have access to free licences for computational software,  After all, most amateurs and high school students aren't producing cutting edge research.</p>
	<p>First, not all institutions (not even all R1 research schools) provide access to programs such as MATLAB. This means that graduate students can be placed in the position of having to purchase software on their own in order to do research (a lot of which involves going through papers and trying to replicate the results). Graduate students are already in a financially difficult situation, and the effect of having to purchase software disproportionately affects students from underprivileged backgrounds.</p>
	<p>, it's not uncommon that people outside academia may want to use the code from research papers. For instance the findings of this <a href="https://arxiv.org/pdf/1508.06576.pdf">paper</a>, which outlined how to render an input image in the style of famous painters, have been widely used by hobbyists and non academics. There are now multiple subreddits and youtube channels devoted specifically to computer generated art, all helping bring new advancements in science to the mainstream.</p>
    
	<h3>What can we do?</h3>
    <p>Think about how </p>
    <p>When possible use open source alternatives. I.e. Python with Numpy and Scipy or Julia instead of MATLAB, sympy instead of Mathematica, etc. The more people using these softwares, the better they become.</p>

	<p class="footer">More writing about my opinions on academia can be found <a href="../">here</a>.</p>
</div>
</body>

</html>
